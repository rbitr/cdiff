{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "characteristic-killing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From scratch python version of sequential NN training\n",
    "# includes a toy demo fitting to some random data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "alleged-traveler",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "npr = lambda x: np.random.randn(*x)\n",
    "from copy import copy\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "accurate-mapping",
   "metadata": {},
   "outputs": [],
   "source": [
    "# each function returns a forward function for the layer\n",
    "# forward functions begin with _\n",
    "# only linear layer has parameters so far (well, cross_entropy has targets)\n",
    "\n",
    "def linear(W, B):\n",
    "    assert(W.shape[-1]==B.shape[0])\n",
    "    d = {\"W\":W, \"B\":B}\n",
    "    def _linear(X):\n",
    "        return np.matmul(X,d[\"W\"])+d[\"B\"]\n",
    "    \n",
    "    return _linear\n",
    "\n",
    "# done for naming conventions, I think there's a better way\n",
    "def actual_relu(x):\n",
    "    if x>0.: return x\n",
    "\n",
    "    return 0.\n",
    "\n",
    "def _relu(X):\n",
    "    return np.vectorize(actual_relu)(X)\n",
    "\n",
    "relu = _relu\n",
    "\n",
    "def _sigmoid(x):\n",
    "    return 0.5 * (np.tanh(x / 2) + 1)\n",
    "\n",
    "sigmoid = _sigmoid \n",
    "\n",
    "def cross_ent(targets):\n",
    "    d = {\"T\":targets}\n",
    "    def _cross_ent(X):\n",
    "        label_probs = X * d[\"T\"] + (1 - X) * (1 - d[\"T\"])\n",
    "        return -np.log(label_probs)\n",
    "    return _cross_ent\n",
    "\n",
    "# helper to turn functional programming into OO\n",
    "def param(layer,p):\n",
    "    return layer.__closure__[0].cell_contents[p]\n",
    "\n",
    "# takes the layer, parameter to differentiate wrt, input and output gradient\n",
    "def vjp(l,p,X,dldy):\n",
    "    if l.__name__ == \"_linear\":\n",
    "        if p=='X':\n",
    "            return np.matmul(dldy,param(l,'W').T)\n",
    "        if p=='W':\n",
    "            return np.matmul(X.T,dldy)\n",
    "        if p=='B':\n",
    "            return np.matmul(np.ones((1,dldy.shape[0])),dldy)\n",
    "            \n",
    "        \n",
    "    if l.__name__ == '_relu':\n",
    "        return (dldy*relu(np.sign(X))).reshape(dldy.shape)\n",
    "    \n",
    "    if l.__name__ == '_sigmoid':\n",
    "        return (dldy*0.5/(np.cosh(X)**2)).reshape(dldy.shape)\n",
    "        \n",
    "    \n",
    "    if l.__name__ == '_cross_ent':\n",
    "        T = param(l,'T')\n",
    "        return (((-T/X + (1-T)/(1-X+.0001)))*dldy).reshape(dldy.shape)\n",
    "    \n",
    "    raise NotImplementedError(f\"Layer: {l.__name__}\")\n",
    "    \n",
    "def nn(layers):\n",
    "    def forward(X):\n",
    "        _X = copy(X)\n",
    "        for l in layers:\n",
    "            _X = l(_X)\n",
    "            \n",
    "        return _X\n",
    "    \n",
    "    return forward\n",
    "\n",
    "def gradient(layers):\n",
    "    def backward(X):\n",
    "        forwards = [X, layers[0](X)] \n",
    "        for ix,l in enumerate(layers[1:]):\n",
    "            forwards.append(l(forwards[ix+1]))\n",
    "                \n",
    "        dldy = np.ones((X.shape[0],1)) #number of points in minibatch\n",
    "        \n",
    "        grads = [dldy]\n",
    "        param_grads = []\n",
    "        \n",
    "        #print(\"forward layers\")\n",
    "        #for f in forwards:\n",
    "            #print(f.shape)\n",
    "        \n",
    "        for l,f in zip(layers[::-1],forwards[-2::-1]):\n",
    "            \n",
    "            grads.append(vjp(l,'X',f,grads[-1])) # f is the layer input\n",
    "            \n",
    "            if hasattr(l,\"__name__\") and l.__name__ == \"_linear\":\n",
    "                # ultimately needs to identify the parameters a layer has and take gradient\n",
    "                param_grads.append(vjp(l,'W',f,grads[-2])) # grads just got one appended earlier\n",
    "                param_grads.append(vjp(l,'B',f,grads[-2]))\n",
    "            \n",
    "        return grads, param_grads\n",
    "            \n",
    "    return backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "native-vatican",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gnuplot(series,markers=itertools.repeat(\"x\"),extra=\"\"):\n",
    "    pts = [\"\\n\".join([f\"{x} {y}\" for (x,y) in s]) for s in series]\n",
    "    plots = \",\".join([f\" '<(echo \\\\\\\"{p}\\\\\\\")' pt \\\\\\\"{m}\\\\\\\" notitle \" for p,m in zip(pts,markers)])\n",
    "    q = f\"\"\" \"set terminal dumb 80 25; {extra}\n",
    "             plot {plots}\"\n",
    "        \"\"\"\n",
    "    !gnuplot -e {q}   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "super-lingerie",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some random input data points and ground truth labels (one of two classes)\n",
    "X0=npr((10,2))\n",
    "T = relu(np.sign(npr((10,1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "retired-mainstream",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\f",
      "        +         +        +         +         +         +        +         +   \r\n",
      "    2 +-+-------------------------------------------------------------------+-+ \r\n",
      "        |                    0                                              |   \r\n",
      "  1.5 +-|                                                                   |-+ \r\n",
      "        |                                                                   |   \r\n",
      "    1 +-|                                                              0    |-+ \r\n",
      "        |                                                                   |   \r\n",
      "        |                                                                   |   \r\n",
      "  0.5 +-|   1                                  0                            |-+ \r\n",
      "        |                         1                                         |   \r\n",
      "    0 +-|                      1                       0                    |-+ \r\n",
      "        |                      0                                            |   \r\n",
      " -0.5 +-|                                                                   |-+ \r\n",
      "        |                                                                   |   \r\n",
      "   -1 +-|                                                                   |-+ \r\n",
      "        |                                                                   |   \r\n",
      "        |   1                                                               |   \r\n",
      " -1.5 +-|                                                                   |-+ \r\n",
      "        |                                                                   |   \r\n",
      "   -2 +-|                                                                   |-+ \r\n",
      "        |             1                                                     |   \r\n",
      " -2.5 +-+-------------------------------------------------------------------+-+ \r\n",
      "        +         +        +         +         +         +        +         +   \r\n",
      "      -1.5       -1      -0.5        0        0.5        1       1.5        2   \r\n",
      "                                                                                \r\n"
     ]
    }
   ],
   "source": [
    "# plot the datapoints by class\n",
    "s1 = [(x,y) for (x,y),t in zip(X0,T) if t == 0]\n",
    "s2 = [(x,y) for (x,y),t in zip(X0,T) if t == 1]\n",
    "gnuplot([s1, s2], \"01\", \"set xtics out; set ytics out;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "tender-mechanism",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random initialization\n",
    "W0 = npr((2,8))\n",
    "B0 = npr((8,))\n",
    "\n",
    "W1 = npr((8,1))\n",
    "B1 = npr((1,))\n",
    "\n",
    "# two layer NN\n",
    "net = [linear(W0,B0),\n",
    "       relu,\n",
    "       linear(W1,B1),\n",
    "       sigmoid,\n",
    "       cross_ent(T),\n",
    "      ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "important-palestine",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ground truth: x\n",
      "prediction: o\n",
      "\f",
      "           +             +             +            +             +             \n",
      "  1.2 +-+-------------------------------------------------------------------+-+ \n",
      "        |                                                                   |   \n",
      "        |                                                                   |   \n",
      "    1 +-|                x             x            x      x             x  |-+ \n",
      "        |                                                                   |   \n",
      "        |                                                                   |   \n",
      "  0.8 +-|                                                                   |-+ \n",
      "        |                                                                   |   \n",
      "        |                                                                   |   \n",
      "  0.6 +-|                                    o                              |-+ \n",
      "        |                                           o                       |   \n",
      "  0.4 +-|                                                                   |-+ \n",
      "        |                o                                 o             o  |   \n",
      "        |                       o                                           |   \n",
      "  0.2 +-|                              o                          o         |-+ \n",
      "        |                                                                   |   \n",
      "        |                                                                   |   \n",
      "    0 +-|  o      o             x            x                    x         |-+ \n",
      "        |                                                                   |   \n",
      "        |                                                                   |   \n",
      " -0.2 +-+-------------------------------------------------------------------+-+ \n",
      "           +             +             +            +             +             \n",
      "           0             2             4            6             8             \n",
      "                                                                                \n"
     ]
    }
   ],
   "source": [
    "# plot the initial softmax output vs class labels\n",
    "initial_out = nn(net[:-1])(X0)\n",
    "series = [enumerate(T.flatten()), enumerate(initial_out.flatten())]\n",
    "markers = \"xo\"\n",
    "extra = \"set xtics out; set ytics out; set yrange [-0.2:1.2]; set xrange [-0.5:9.5];\"\n",
    "print (\"ground truth: x\\nprediction: o\")\n",
    "gnuplot(series, markers,extra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "muslim-support",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0, Loss 6.525573170967385\n",
      "Step: 1, Loss 6.197217751425823\n",
      "Step: 2, Loss 5.895874423487789\n",
      "Step: 3, Loss 5.650123918902228\n",
      "Step: 4, Loss 5.4519834954889035\n",
      "Step: 5, Loss 5.283075291050255\n",
      "Step: 6, Loss 5.128004147059727\n",
      "Step: 7, Loss 4.974638743636677\n",
      "Step: 8, Loss 4.842789449354516\n",
      "Step: 9, Loss 4.7284585342144085\n",
      "Step: 10, Loss 4.628280485447879\n",
      "Step: 11, Loss 4.5395105511121585\n",
      "Step: 12, Loss 4.45997599293664\n",
      "Step: 13, Loss 4.400451338474991\n",
      "Step: 14, Loss 4.355767593398162\n",
      "Step: 15, Loss 4.313962246182162\n",
      "Step: 16, Loss 4.274640892293005\n",
      "Step: 17, Loss 4.237485227484032\n",
      "Step: 18, Loss 4.202235548285703\n",
      "Step: 19, Loss 4.168677597540788\n",
      "Step: 20, Loss 4.136632622935657\n",
      "Step: 21, Loss 4.10594982565236\n",
      "Step: 22, Loss 4.076500597006351\n",
      "Step: 23, Loss 4.04817409920334\n",
      "Step: 24, Loss 4.020873860762831\n",
      "Step: 25, Loss 3.9945151406359667\n",
      "Step: 26, Loss 3.969022876489216\n",
      "Step: 27, Loss 3.9443300781905815\n",
      "Step: 28, Loss 3.9203765615311355\n",
      "Step: 29, Loss 3.897107942699872\n",
      "Step: 30, Loss 3.8753171126100505\n",
      "Step: 31, Loss 3.856704304669046\n",
      "Step: 32, Loss 3.8389807227900583\n",
      "Step: 33, Loss 3.8220555748317415\n",
      "Step: 34, Loss 3.805850200109401\n",
      "Step: 35, Loss 3.7902959899007027\n",
      "Step: 36, Loss 3.775332740732596\n",
      "Step: 37, Loss 3.7609073391194667\n",
      "Step: 38, Loss 3.7469727018982373\n",
      "Step: 39, Loss 3.7334869150411194\n",
      "Step: 40, Loss 3.7204125276731004\n",
      "Step: 41, Loss 3.7077159683118186\n",
      "Step: 42, Loss 3.695367058040408\n",
      "Step: 43, Loss 3.6833386011092037\n",
      "Step: 44, Loss 3.6716060378386652\n",
      "Step: 45, Loss 3.6601471480255072\n",
      "Step: 46, Loss 3.6489417956010675\n",
      "Step: 47, Loss 3.6379717072493807\n",
      "Step: 48, Loss 3.6272202792055306\n",
      "Step: 49, Loss 3.616672407629056\n",
      "Step: 50, Loss 3.606314338862199\n",
      "Step: 51, Loss 3.5961335365984204\n",
      "Step: 52, Loss 3.58611856354838\n",
      "Step: 53, Loss 3.5762589756329457\n",
      "Step: 54, Loss 3.566545227082335\n",
      "Step: 55, Loss 3.5569685850975037\n",
      "Step: 56, Loss 3.5475210529503296\n",
      "Step: 57, Loss 3.538195300574994\n",
      "Step: 58, Loss 3.528984601844173\n",
      "Step: 59, Loss 3.5198827778372825\n",
      "Step: 60, Loss 3.5108841455003676\n",
      "Step: 61, Loss 3.501983471172474\n",
      "Step: 62, Loss 3.493175928515441\n",
      "Step: 63, Loss 3.484457060435627\n",
      "Step: 64, Loss 3.475822744629425\n",
      "Step: 65, Loss 3.4672691624212457\n",
      "Step: 66, Loss 3.4587927705943375\n",
      "Step: 67, Loss 3.450390275942347\n",
      "Step: 68, Loss 3.442058612293796\n",
      "Step: 69, Loss 3.433794919783212\n",
      "Step: 70, Loss 3.425596526162046\n",
      "Step: 71, Loss 3.4174609299600345\n",
      "Step: 72, Loss 3.4093857853237024\n",
      "Step: 73, Loss 3.40136888837335\n",
      "Step: 74, Loss 3.393408164933349\n",
      "Step: 75, Loss 3.3855016595030865\n",
      "Step: 76, Loss 3.377647525347359\n",
      "Step: 77, Loss 3.3698440155957003\n",
      "Step: 78, Loss 3.3620894752499826\n",
      "Step: 79, Loss 3.354382334008746\n",
      "Step: 80, Loss 3.346721099825114\n",
      "Step: 81, Loss 3.3391043531229294\n",
      "Step: 82, Loss 3.3315307416029003\n",
      "Step: 83, Loss 3.3239989755770942\n",
      "Step: 84, Loss 3.3165078237761394\n",
      "Step: 85, Loss 3.309056109579039\n",
      "Step: 86, Loss 3.301642707620502\n",
      "Step: 87, Loss 3.2942665407352862\n",
      "Step: 88, Loss 3.2869265772032388\n",
      "Step: 89, Loss 3.2796218282624823\n",
      "Step: 90, Loss 3.272351345861625\n",
      "Step: 91, Loss 3.265114220624971\n",
      "Step: 92, Loss 3.25790958000749\n",
      "Step: 93, Loss 3.250736586618819\n",
      "Step: 94, Loss 3.243594436697818\n",
      "Step: 95, Loss 3.2364823587212177\n",
      "Step: 96, Loss 3.2293996121317203\n",
      "Step: 97, Loss 3.2223454861725003\n",
      "Step: 98, Loss 3.2153192988165062\n",
      "Step: 99, Loss 3.2083203957802424\n",
      "Step: 100, Loss 3.201348149612841\n",
      "Step: 101, Loss 3.1944019588522266\n",
      "Step: 102, Loss 3.187980231773063\n",
      "Step: 103, Loss 3.1820704263396804\n",
      "Step: 104, Loss 3.176190600677026\n",
      "Step: 105, Loss 3.170339827763613\n",
      "Step: 106, Loss 3.164517278166855\n",
      "Step: 107, Loss 3.15872220916369\n",
      "Step: 108, Loss 3.1529539551458203\n",
      "Step: 109, Loss 3.147211919145545\n",
      "Step: 110, Loss 3.141495565339774\n",
      "Step: 111, Loss 3.135804412408437\n",
      "Step: 112, Loss 3.13013802763944\n",
      "Step: 113, Loss 3.124496021686205\n",
      "Step: 114, Loss 3.118878043895702\n",
      "Step: 115, Loss 3.113283778135323\n",
      "Step: 116, Loss 3.1077129390558724\n",
      "Step: 117, Loss 3.102165268735841\n",
      "Step: 118, Loss 3.0967394570127813\n",
      "Step: 119, Loss 3.0916033721232026\n",
      "Step: 120, Loss 3.0864705827113914\n",
      "Step: 121, Loss 3.0813426610778314\n",
      "Step: 122, Loss 3.0782351982251557\n",
      "Step: 123, Loss 3.068362267329788\n",
      "Step: 124, Loss 3.0649608674060294\n",
      "Step: 125, Loss 3.056420285201532\n",
      "Step: 126, Loss 3.0528440949918805\n",
      "Step: 127, Loss 3.0452942428862633\n",
      "Step: 128, Loss 3.0416562402656417\n",
      "Step: 129, Loss 3.034818331643558\n",
      "Step: 130, Loss 3.0312604159935583\n",
      "Step: 131, Loss 3.0257870414953763\n",
      "Step: 132, Loss 3.0223259816808867\n",
      "Step: 133, Loss 3.0162575089138524\n",
      "Step: 134, Loss 3.012962091934655\n",
      "Step: 135, Loss 3.00603121475349\n",
      "Step: 136, Loss 3.002011595490176\n",
      "Step: 137, Loss 2.9950774753371046\n",
      "Step: 138, Loss 2.991392532782558\n",
      "Step: 139, Loss 2.9843611560178487\n",
      "Step: 140, Loss 2.9821255498382384\n",
      "Step: 141, Loss 2.9749537609471073\n",
      "Step: 142, Loss 2.9722340412359474\n",
      "Step: 143, Loss 2.964786260949803\n",
      "Step: 144, Loss 2.962621886993921\n",
      "Step: 145, Loss 2.9548144394093523\n",
      "Step: 146, Loss 2.953264396721122\n",
      "Step: 147, Loss 2.9450214855195758\n",
      "Step: 148, Loss 2.9441664369358085\n",
      "Step: 149, Loss 2.9369756291594067\n",
      "Step: 150, Loss 2.9308253157311146\n",
      "Step: 151, Loss 2.928527725945193\n",
      "Step: 152, Loss 2.921624516790442\n",
      "Step: 153, Loss 2.9202199210317974\n",
      "Step: 154, Loss 2.9125273750846503\n",
      "Step: 155, Loss 2.912044811524846\n",
      "Step: 156, Loss 2.9043356646509713\n",
      "Step: 157, Loss 2.898377401911467\n",
      "Step: 158, Loss 2.8978993058225373\n",
      "Step: 159, Loss 2.8907793248496754\n",
      "Step: 160, Loss 2.890309960814329\n",
      "Step: 161, Loss 2.8829472333339874\n",
      "Step: 162, Loss 2.877100167242945\n",
      "Step: 163, Loss 2.8757775559336345\n",
      "Step: 164, Loss 2.868724983752263\n",
      "Step: 165, Loss 2.8696005283780117\n",
      "Step: 166, Loss 2.8644819324791255\n",
      "Step: 167, Loss 2.8591382243477463\n",
      "Step: 168, Loss 2.859069948570658\n",
      "Step: 169, Loss 2.852872092310874\n",
      "Step: 170, Loss 2.8483108486795583\n",
      "Step: 171, Loss 2.847692790170889\n",
      "Step: 172, Loss 2.841660132261133\n",
      "Step: 173, Loss 2.842691065500657\n",
      "Step: 174, Loss 2.8379175984494545\n",
      "Step: 175, Loss 2.832482760018025\n",
      "Step: 176, Loss 2.832842933882314\n",
      "Step: 177, Loss 2.8269229616919036\n",
      "Step: 178, Loss 2.8219459773702122\n",
      "Step: 179, Loss 2.8220078703560274\n",
      "Step: 180, Loss 2.8163987155377073\n",
      "Step: 181, Loss 2.8131719139343\n",
      "Step: 182, Loss 2.8130216645768633\n",
      "Step: 183, Loss 2.8072910448332067\n",
      "Step: 184, Loss 2.802797773619426\n",
      "Step: 185, Loss 2.802562153590787\n",
      "Step: 186, Loss 2.7969205000330715\n",
      "Step: 187, Loss 2.792817502692954\n",
      "Step: 188, Loss 2.794062754032259\n",
      "Step: 189, Loss 2.7884675358042057\n",
      "Step: 190, Loss 2.78400039074951\n",
      "Step: 191, Loss 2.7838305949917337\n",
      "Step: 192, Loss 2.7783072345682056\n",
      "Step: 193, Loss 2.7738597948734207\n",
      "Step: 194, Loss 2.7756530744982064\n",
      "Step: 195, Loss 2.770154254750035\n",
      "Step: 196, Loss 2.765341994789643\n",
      "Step: 197, Loss 2.7655436572495504\n",
      "Step: 198, Loss 2.7601053726774714\n",
      "Step: 199, Loss 2.7550738900594913\n"
     ]
    }
   ],
   "source": [
    "# train the model with sgd\n",
    "\n",
    "α = -0.01\n",
    "losses = []\n",
    "\n",
    "for step in range(200):\n",
    "\n",
    "    L = sum(nn(net)(X0))[0]\n",
    "    losses.append(L)\n",
    "    \n",
    "    _, dLdθ = gradient(net)(X0)\n",
    "    \n",
    "    W0 += α*dLdθ[2] #npr((2,8))\n",
    "    B0 += α*dLdθ[3].reshape((B0.shape))#npr((8,))\n",
    "\n",
    "    W1 += α*dLdθ[0]#npr((8,1))\n",
    "    B1 += α*dLdθ[1].reshape((B1.shape))#npr((1,))\n",
    "\n",
    "    net = [linear(W0,B0),\n",
    "           relu,\n",
    "           linear(W1,B1),\n",
    "           sigmoid,\n",
    "           cross_ent(T),\n",
    "          ]\n",
    "\n",
    "    print(f\"Step: {step}, Loss {L}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "conservative-tours",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\f",
      "                                                                                \r\n",
      "    7 +---------------------------------------------------------------------+   \r\n",
      "      |      +      +      +      +      +      +      +      +      +      |   \r\n",
      "  6.5 |-+                                                                 +-|   \r\n",
      "      |                                                                     |   \r\n",
      "      |                                                                     |   \r\n",
      "    6 |x+                                                                 +-|   \r\n",
      "      |x                                                                    |   \r\n",
      "  5.5 |x+                                                                 +-|   \r\n",
      "      | x                                                                   |   \r\n",
      "    5 |-x                                                                 +-|   \r\n",
      "      |  x                                                                  |   \r\n",
      "      |  xx                                                                 |   \r\n",
      "  4.5 |-+ xx                                                              +-|   \r\n",
      "      |    xxx                                                              |   \r\n",
      "    4 |-+    xxxx                                                         +-|   \r\n",
      "      |          xxxxx                                                      |   \r\n",
      "  3.5 |-+            xxxxxxxxx                                            +-|   \r\n",
      "      |                      xxxxxxxxxxx                                    |   \r\n",
      "      |                                xxxxxxxxxxxxx                        |   \r\n",
      "    3 |-+                                          xxxxxxxxxxxxxxxxxxx    +-|   \r\n",
      "      |      +      +      +      +      +      +      +      +      xxxxxxx|   \r\n",
      "  2.5 +---------------------------------------------------------------------+   \r\n",
      "      0      20     40     60     80    100    120    140    160    180    200  \r\n",
      "                                                                                \r\n"
     ]
    }
   ],
   "source": [
    "# loss\n",
    "gnuplot([[(x,y) for x,y in enumerate(losses)]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "expensive-alcohol",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ground truth: x\n",
      "prediction: o\n",
      "\f",
      "           +             +             +            +             +             \n",
      "  1.2 +-+-------------------------------------------------------------------+-+ \n",
      "        |                                                                   |   \n",
      "        |                                                                   |   \n",
      "    1 +-|                x             x            x      x             x  |-+ \n",
      "        |                                           o                       |   \n",
      "        |                o                                                  |   \n",
      "  0.8 +-|                                                                o  |-+ \n",
      "        |                                                                   |   \n",
      "        |                              o     o             o                |   \n",
      "  0.6 +-|                                                                   |-+ \n",
      "        |                                                                   |   \n",
      "  0.4 +-|                                                                   |-+ \n",
      "        |                                                                   |   \n",
      "        |                                                                   |   \n",
      "  0.2 +-|                                                                   |-+ \n",
      "        |         o                                               o         |   \n",
      "        |                       o                                           |   \n",
      "    0 +-|  o      x             x            x                    x         |-+ \n",
      "        |                                                                   |   \n",
      "        |                                                                   |   \n",
      " -0.2 +-+-------------------------------------------------------------------+-+ \n",
      "           +             +             +            +             +             \n",
      "           0             2             4            6             8             \n",
      "                                                                                \n"
     ]
    }
   ],
   "source": [
    "# compate how the outputs have moved towards the ground truth \n",
    "# for the trained network (same plot as before)\n",
    "trained_out = nn(net[:-1])(X0)\n",
    "series = [enumerate(T.flatten()), enumerate(trained_out.flatten())]\n",
    "markers = \"xo\"\n",
    "extra = \"set xtics out; set ytics out; set yrange [-0.2:1.2]; set xrange [-0.5:9.5];\"\n",
    "print (\"ground truth: x\\nprediction: o\")\n",
    "gnuplot(series, markers,extra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cognitive-lancaster",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fluid-saskatchewan",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
